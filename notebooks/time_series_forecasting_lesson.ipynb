{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61dc1278",
   "metadata": {},
   "source": [
    "# Introduction to Time-Series Data\n",
    "\n",
    "Time-series data is a sequence of data points collected or recorded at time-ordered intervals. Examples include daily stock prices, monthly sales data, or yearly temperature readings. Analyzing time-series data enables us to understand underlying patterns such as trends, seasonality, and cyclical fluctuations. \n",
    "\n",
    "Note the difference between time-series data and cross-sectional data below. So far we have been dealing with cross-sectional data.\n",
    "\n",
    "![time-series-cross-sectional](../assets/cross_sectional_time_series.png)\n",
    "\n",
    "## Trend, Season, Cycle\n",
    "\n",
    "![antidiabetic_drug](../assets/antidiabetic_drug_sales.png)\n",
    "\n",
    "There are 3 components of time-series data:\n",
    "\n",
    "- Trend\n",
    "\n",
    "A trend exists when there is a long-term increase or decrease in the data. It does not have to be linear. Sometimes we will refer to a trend as “changing direction”, when it might go from an increasing trend to a decreasing trend. There is a trend in the antidiabetic drug sales data shown.\n",
    "\n",
    "- Seasonal\n",
    "\n",
    "A seasonal pattern occurs when a time series is affected by seasonal factors such as the time of the year or the day of the week. Seasonality is always of a fixed and known period. The monthly sales of antidiabetic drugs shows seasonality which is induced partly by the change in the cost of the drugs at the end of the calendar year.\n",
    "\n",
    "- Cyclic\n",
    "\n",
    "A cycle occurs when the data exhibit rises and falls that are not of a fixed frequency. These fluctuations are usually due to economic conditions, and are often related to the “business cycle”. The duration of these fluctuations is usually at least 2 years.\n",
    "\n",
    "> Many people confuse cyclic behaviour with seasonal behaviour, but they are really quite different. If the fluctuations are not of a fixed frequency then they are cyclic; if the frequency is unchanging and associated with some aspect of the calendar, then the pattern is seasonal. In general, the average length of cycles is longer than the length of a seasonal pattern, and the magnitudes of cycles tend to be more variable than the magnitudes of seasonal patterns.\n",
    "\n",
    "Many time series include trend, cycles and seasonality. When choosing a forecasting method, we will first need to identify the time series patterns in the data, and then choose a method that is able to capture the patterns properly.\n",
    "\n",
    "### Examples\n",
    "\n",
    "![four_examples](../assets/four_examples.png)\n",
    "\n",
    "1. The monthly housing sales (top left) show strong seasonality within each year, as well as some strong cyclic behaviour with a period of about 6–10 years. There is no apparent trend in the data over this period.\n",
    "2. The US treasury bill contracts (top right) show results from the Chicago market for 100 consecutive trading days in 1981. Here there is no seasonality, but an obvious downward trend. Possibly, if we had a much longer series, we would see that this downward trend is actually part of a long cycle, but when viewed over only 100 days it appears to be a trend.\n",
    "3. The Australian quarterly electricity production (bottom left) shows a strong increasing trend, with strong seasonality. There is no evidence of any cyclic behaviour here.\n",
    "4. The daily change in the Google closing stock price (bottom right) has no trend, seasonality or cyclic behaviour. There are random fluctuations which do not appear to be very predictable, and no strong patterns that would help with developing a forecasting model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e6ff05",
   "metadata": {},
   "source": [
    "# Visualizing Time-Series Data\n",
    "\n",
    "Visualizing time-series data is a critical step in the analysis process. It helps in understanding underlying patterns, trends, seasonality, and the presence of noise.\n",
    "\n",
    "## Time Series Plots\n",
    "\n",
    "A time series plot is a graph where some measure of time is the unit on the x-axis, and the variable or variables in which we are interested are plotted on the y-axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b3ab6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sktime.datasets import load_airline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaefce61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8254b8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_airline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d5f4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ff5096",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.plot()\n",
    "plt.title('Time Series Plot')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel(data.name)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b87725e",
   "metadata": {},
   "source": [
    "## Seasonal Plots\n",
    "\n",
    "Seasonal plots are used to assess the presence and type of seasonality in a time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffcec53",
   "metadata": {},
   "outputs": [],
   "source": [
    "years = data.index.year.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e39656",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "for year in years:\n",
    "    plt.plot(data[data.index.year == year].index.month, data[data.index.year == year], label=year)\n",
    "\n",
    "plt.title('Seasonal Plot')\n",
    "plt.xlabel('Month of Year')\n",
    "plt.xticks(range(1, 13))\n",
    "plt.ylabel(data.name)\n",
    "plt.legend(title='Year')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd57b207",
   "metadata": {},
   "source": [
    "## Subseries Plots\n",
    "\n",
    "Subseries plots (or seasonal subseries plots) can be useful to compare seasonal patterns across different years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a6ecc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbb78fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = data.index.astype('datetime64')\n",
    "data_df = data.reset_index(name='value')\n",
    "data_df['month'] = dates.strftime('%b')\n",
    "data_df['year'] = dates.year.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38266ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578bd6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(data_df, col=\"month\", col_wrap=3, height=3, aspect=1.5, sharey=False)\n",
    "g.map(sns.lineplot, \"year\", \"value\", marker='o')\n",
    "g.set_axis_labels('Year', 'Number of Passengers')\n",
    "g.set_titles(\"{col_name}\")\n",
    "g.fig.suptitle('Subseries Plot of Monthly Airline Passengers by Year', y=1.05)\n",
    "g.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8440979f",
   "metadata": {},
   "source": [
    "## Lag Plots\n",
    "\n",
    "Lag plots are used to check for autocorrelation in a time series. If the data are random, the lag plot will exhibit no identifiable pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9565bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import lag_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d319e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 6))\n",
    "lag_plot(data, lag=1)\n",
    "plt.title('Lag Plot with lag=1')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9dd17e6",
   "metadata": {},
   "source": [
    "## Autocorrelation\n",
    "\n",
    "Autocorrelation, also known as serial correlation, is a statistical measure that quantifies the degree of similarity between a time series and a lagged version of itself over successive time intervals. It's a tool commonly used in time-series analysis to help understand the internal structure of the data.\n",
    "\n",
    "Autocorrelation measures the relationship between a variable's current value and its past values. It is an important feature of time-series data, which often depends on previous time points. For instance, today's stock market price is likely to be similar to yesterday's price because economic factors change gradually over time.\n",
    "\n",
    "Positive autocorrelation occurs when an increase in a time series leads to a proportional increase in a lagged version of itself. Negative autocorrelation is when an increase leads to a proportional decrease. If there is no autocorrelation, the time series does not have any linear relationship with its lagged versions.\n",
    "\n",
    "### Autocorrelation Function (ACF)\n",
    "\n",
    "The autocorrelation function (ACF) is a function that represents autocorrelation of a time series as a function of the time lag. It gives us values of autocorrelation at different lags, which can be plotted to show the autocorrelation structure of the data.\n",
    "\n",
    "The ACF at lag $ k $ is calculated as:\n",
    "\n",
    "$$ ACF(k) = \\frac{\\sum_{t=k+1}^{T}(y_t - \\bar{y})(y_{t-k} - \\bar{y})}{\\sum_{t=1}^{T}(y_t - \\bar{y})^2} $$\n",
    "\n",
    "where:\n",
    "- $ y_t $ is the value of the time series at time $ t $\n",
    "- $ \\bar{y} $ is the mean of the time series\n",
    "- $ T $ is the total number of observations\n",
    "\n",
    "The ACF value ranges between -1 and 1. A value close to 1 indicates strong positive autocorrelation, while a value close to -1 indicates strong negative autocorrelation. A value near 0 suggests that there is no autocorrelation.\n",
    "\n",
    "## Autocorrelation (ACF) Plots\n",
    "\n",
    "To visualize the ACF, you can plot it against the lag values. This plot is known as a correlogram or ACF plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f34e9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import acf\n",
    "from statsmodels.graphics.tsaplots import plot_acf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1926863e",
   "metadata": {},
   "outputs": [],
   "source": [
    "autocorrelations = acf(data, nlags=60)\n",
    "print(f'Autocorrelations: {autocorrelations}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300f469a",
   "metadata": {},
   "source": [
    "\n",
    "`lags=20` sets the number of lags to 20, and `alpha=0.05` sets the confidence intervals to 95%. The shaded area in the plot indicates the confidence interval; autocorrelation values outside this area are statistically significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e69ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plot_acf(data, lags=20, alpha=0.05)\n",
    "plt.title('Autocorrelation Function (ACF) Plot')\n",
    "plt.xlabel('Lags')\n",
    "plt.ylabel('Autocorrelation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524ffbb5",
   "metadata": {},
   "source": [
    "## White Noise\n",
    "\n",
    "White noise is a random signal having equal intensity at different frequencies, giving it a constant power spectral density. In time series, white noise is a sequence of random data where every value has a time-independent mean and variance, and zero autocorrelation at all lags.\n",
    "\n",
    "### Identifying White Noise\n",
    "\n",
    "You can identify white noise in a time series by checking the following conditions:\n",
    "\n",
    "- The mean of the series should not be significantly different from zero.\n",
    "- The variance should not change over time.\n",
    "- The autocorrelation at all lags should be near zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd51995",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Mean: {data.mean()}')\n",
    "print(f'Variance: {data.var()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7f655c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import q_stat\n",
    "from scipy.stats import shapiro, levene\n",
    "import numpy as np\n",
    "\n",
    "def is_white_noise(series, lags=20, alpha=0.05):\n",
    "    # 1. Autocorrelation check (Ljung-Box)\n",
    "    _, pvals = q_stat(acf(series, nlags=lags)[1:], len(series))\n",
    "    acf_ok = all(p > alpha for p in pvals)\n",
    "    \n",
    "    # 2. Constant variance check (Levene's test on halves)\n",
    "    split = len(series)//2\n",
    "    _, pval_var = levene(series[:split], series[split:])\n",
    "    var_ok = pval_var > alpha\n",
    "    \n",
    "    # 3. Normality check (Shapiro-Wilk)\n",
    "    _, pval_norm = shapiro(series)\n",
    "    norm_ok = pval_norm > alpha\n",
    "    \n",
    "    return all([acf_ok, var_ok, norm_ok])\n",
    "\n",
    "is_white_noise(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf098d1",
   "metadata": {},
   "source": [
    "# Time Series Decomposition\n",
    "\n",
    "Time series decomposition is a technique that splits a time series into several components, each representing an underlying pattern. By decomposing a time series, we can identify and measure the different factors that influence the data, such as trends, seasonality, and irregular fluctuations.\n",
    "\n",
    "The main components of time series decomposition are:\n",
    "\n",
    "- **Trend**: The increasing or decreasing value in the series.\n",
    "- **Seasonality**: The repeating short-term cycle in the series.\n",
    "- **Residuals**: The random variation in the series.\n",
    "\n",
    "## Types of Decomposition\n",
    "\n",
    "There are two primary types of decomposition methods:\n",
    "\n",
    "1. **Additive Decomposition**: Assumes that the components add together to make the time series.\n",
    "   \n",
    "   $$ Y_t = T_t + S_t + R_t $$\n",
    "\n",
    "2. **Multiplicative Decomposition**: Assumes that the components multiply together to make the time series.\n",
    "   \n",
    "   $$ Y_t = T_t \\times S_t \\times R_t $$\n",
    "\n",
    "The choice between additive and multiplicative decomposition depends on the nature of the seasonal variation. If the seasonal variation is roughly constant throughout the series, an additive model is appropriate. If the seasonal variation increases over time, a multiplicative model is more suitable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c85c127",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d89c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.index = dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d41fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform additive decomposition\n",
    "decomposition = seasonal_decompose(data, model='additive')\n",
    "\n",
    "# Plot the decomposed components\n",
    "fig = decomposition.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1caecdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform multiplicative decomposition\n",
    "decomposition = seasonal_decompose(data, model='multiplicative')\n",
    "\n",
    "# Plot the decomposed components\n",
    "fig = decomposition.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67994656",
   "metadata": {},
   "source": [
    "## Interpreting Decomposition Results\n",
    "\n",
    "After decomposing the time series, you can analyze each component separately:\n",
    "\n",
    "- **Trend**: Look for any long-term upward or downward movement in the data. Smoothing techniques like moving averages can help identify the trend.\n",
    "- **Seasonality**: Identify any regular pattern that repeats over a fixed period, such as days, months, or quarters.\n",
    "- **Residuals**: Examine the leftover or error component after removing the trend and seasonality. Ideally, the residuals should be random and have a mean close to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96721b32",
   "metadata": {},
   "source": [
    "# Time-Series Forecasting / Modelling Evaluation\n",
    "\n",
    "In time series analysis, the order of observations is crucial. Unlike random train-test splits used in cross-sectional data, time series data requires careful handling to preserve the temporal order during model training and evaluation. Temporal train-test split and time series cross-validation are techniques designed to respect the time order of observations.\n",
    "\n",
    "![temporal_split](../assets/temporal_split.png)\n",
    "\n",
    "## Temporal Train-Test Split\n",
    "\n",
    "Temporal train-test split is a method where the dataset is divided into training and test sets based on time. The training set consists of initial observations, and the test set consists of subsequent observations.\n",
    "\n",
    "### Why Use Temporal Train-Test Split?\n",
    "\n",
    "- To prevent future information from leaking into the model training process.\n",
    "- To evaluate the model's performance on unseen future data.\n",
    "\n",
    "## Time Series Cross-Validation\n",
    "\n",
    "Time series cross-validation is a resampling technique used to evaluate time series models. It involves multiple train-test splits, each time with a different test set.\n",
    "\n",
    "### Types of Time Series Cross-Validation\n",
    "\n",
    "- **Sliding Window**: The training set slides forward in time, expanding to include the next observation.\n",
    "- **Expanding Window**: The training set expands to include all available data up to the next test set.\n",
    "\n",
    "### Advantages of Time Series Cross-Validation\n",
    "\n",
    "- Provides a more robust estimate of the model's performance.\n",
    "- Utilizes more data for training, which can be beneficial for small datasets.\n",
    "\n",
    "### Challenges of Time Series Cross-Validation\n",
    "\n",
    "- Computationally expensive due to multiple train-test splits.\n",
    "- Choosing the correct window size and forecasting horizon can be challenging."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6978fbe9",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "1. **Mean Absolute Error (MAE)**:\n",
    "   - Explanation: The MAE measures the average magnitude of the errors in a set of forecasts, without considering their direction. It's the average over the test sample of the absolute differences between prediction and actual observation where all individual differences have equal weight.\n",
    "     $$ \\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i| $$\n",
    "\n",
    "2. **Mean Squared Error (MSE)**:\n",
    "   - Explanation: The MSE measures the average of the squares of the errors—that is, the average squared difference between the estimated values and the actual value. MSE is more sensitive to outliers than MAE because it squares the errors.\n",
    "     $$ \\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 $$\n",
    "\n",
    "3. **Root Mean Squared Error (RMSE)**:\n",
    "   - Explanation: The RMSE is the square root of the MSE and serves to scale the error to the same units as the forecasted variable. It gives a relatively high weight to large errors.\n",
    "     $$ \\text{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2} $$\n",
    "\n",
    "4. **Mean Absolute Percentage Error (MAPE)**:\n",
    "   - Explanation: The MAPE measures the size of the error in percentage terms. It is calculated as the average of the absolute percentage errors of the forecasts.\n",
    "     $$ \\text{MAPE} = \\frac{100\\%}{n} \\sum_{i=1}^{n} \\left| \\frac{y_i - \\hat{y}_i}{y_i} \\right| $$\n",
    "\n",
    "5. **Symmetric Mean Absolute Percentage Error (sMAPE)**:\n",
    "   - Explanation: The sMAPE is a variation of the MAPE that addresses some of its limitations, such as being undefined when $ y_i $ is zero. The sMAPE is symmetric because it equally penalizes positive and negative forecast errors.\n",
    "     $$ \\text{sMAPE} = \\frac{100\\%}{n} \\sum_{i=1}^{n} \\frac{|y_i - \\hat{y}_i|}{(|y_i| + |\\hat{y}_i|)/2} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82a33ed",
   "metadata": {},
   "source": [
    "# Naive Forecasting\n",
    "\n",
    "Naive forecasting methods are simple yet surprisingly effective for making predictions in time series analysis. They are based on the assumption that the most recent observations are the best predictors of the future. These methods serve as a good starting point and baseline for comparing more complex models.\n",
    "\n",
    "## What is Naive Forecasting?\n",
    "\n",
    "Naive forecasting uses the value from the last period as the prediction for the next period. It is called \"naive\" because it assumes the simplest form of prediction without accounting for trends, seasonality, or other patterns in the data.\n",
    "\n",
    "## The Naive Forecasting Model\n",
    "\n",
    "The naive forecasting model can be mathematically represented as:\n",
    "\n",
    "$$ \\hat{y}_{t+1} = y_t $$\n",
    "\n",
    "where:\n",
    "- $ \\hat{y}_{t+1} $ is the predicted value for the next period\n",
    "- $ y_t $ is the observed value at time $ t $\n",
    "\n",
    "## Variations of Naive Forecasting\n",
    "\n",
    "### Seasonal Naive Forecasting\n",
    "\n",
    "The seasonal naive method takes the last observation from the same season of the previous cycle as the forecast. This is useful when the data exhibits strong seasonality.\n",
    "\n",
    "The seasonal naive forecast can be expressed as:\n",
    "\n",
    "$$ \\hat{y}_{t+1} = y_{t+1-m} $$\n",
    "\n",
    "where:\n",
    "- $ m $ is the seasonal period length\n",
    "\n",
    "### Drift Method\n",
    "\n",
    "The drift method allows the forecasts to change over time at the average rate of change seen in the historical data. It is represented as:\n",
    "\n",
    "$$ \\hat{y}_{t+h} = y_t + h \\left( \\frac{y_t - y_1}{t-1} \\right) $$\n",
    "\n",
    "where:\n",
    "- $ h $ is the forecast horizon\n",
    "- $ y_1 $ is the first observed value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96dc6bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.forecasting.naive import NaiveForecaster\n",
    "from sktime.utils.plotting import plot_series\n",
    "from sktime.forecasting.model_selection import temporal_train_test_split\n",
    "from sktime.performance_metrics.forecasting import mean_absolute_percentage_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2098a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = load_airline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96201397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the airline passengers data into train and test\n",
    "# the last 3 years will be the test set, and the prior years are the training set\n",
    "y_train, y_test = temporal_train_test_split(y, test_size=36)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4602e8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbe6287",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1679817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the naive forecaster\n",
    "forecaster = NaiveForecaster(strategy=\"last\")\n",
    "forecaster.fit(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99db183c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the forecasting horizon\n",
    "fh = np.arange(1, len(y_test) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ec2064",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = forecaster.predict(fh=fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7641634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting predictions and past data\n",
    "plot_series(y_test, y_pred, labels=[\"y\", \"y_pred\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ace4bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell demonstrates Seasonal Naïve Forecasting\n",
    "# The seasonal naïve method uses the value from the same season in the previous cycle\n",
    "# (e.g., same month last year) as the forecast. This is useful for data with \n",
    "# strong seasonal patterns.\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sktime.forecasting.naive import NaiveForecaster\n",
    "from sktime.forecasting.base import ForecastingHorizon\n",
    "\n",
    "# assume y_train, y_test share a DateTimeIndex\n",
    "fh = ForecastingHorizon(\n",
    "    np.arange(1, len(y_test) + 1), is_relative=True\n",
    ")\n",
    "\n",
    "forecaster = NaiveForecaster(strategy=\"last\", sp=12)  # seasonal naïve\n",
    "forecaster.fit(y_train)\n",
    "y_pred = forecaster.predict(fh)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 3))\n",
    "y_train.plot(ax=ax, label=\"y_train\")\n",
    "y_test.plot(ax=ax, label=\"y_test\")\n",
    "y_pred.plot(ax=ax, label=\"seasonal naïve pred\")\n",
    "ax.set_ylabel(\"Number of airline passengers\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8da8369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rolling-Origin Forecasting Demonstration\n",
    "# This code shows how naive forecasting works where we predict\n",
    "# one time step ahead, then update our model with the actual value when it arrives.\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sktime.forecasting.naive import NaiveForecaster\n",
    "\n",
    "forecaster = NaiveForecaster(strategy=\"last\")\n",
    "forecaster.fit(y_train)\n",
    "\n",
    "history = y_train.copy()\n",
    "pred_vals = []\n",
    "\n",
    "for stamp in y_test.index:\n",
    "    pred_vals.append(history.iloc[-1])          # forecast using current history\n",
    "    history.loc[stamp] = y_test.loc[stamp]      # once actual arrives, add it\n",
    "\n",
    "y_pred = pd.Series(pred_vals, index=y_test.index)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 3))\n",
    "y_train.plot(ax=ax, label=\"y_train\")\n",
    "y_test.plot(ax=ax, label=\"y_test\")\n",
    "y_pred.plot(ax=ax, label=\"rolling-origin pred\")\n",
    "ax.set_ylabel(\"Number of airline passengers\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfd4c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_absolute_percentage_error(y_test, y_pred, symmetric=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b18de96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the seasonal naive forecaster\n",
    "forecaster = NaiveForecaster(strategy=\"last\", sp=12)\n",
    "forecaster.fit(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccebbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seasonal naive baseline: repeat last year's pattern and plot against actuals\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sktime.forecasting.naive import NaiveForecaster\n",
    "from sktime.forecasting.base import ForecastingHorizon\n",
    "\n",
    "# assume y_train, y_test share a DateTimeIndex\n",
    "fh = ForecastingHorizon(\n",
    "    np.arange(1, len(y_test) + 1), is_relative=True\n",
    ")\n",
    "\n",
    "forecaster = NaiveForecaster(strategy=\"last\", sp=12)  # seasonal naïve\n",
    "forecaster.fit(y_train)\n",
    "y_pred = forecaster.predict(fh)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 3))\n",
    "y_train.plot(ax=ax, label=\"y_train\")\n",
    "y_test.plot(ax=ax, label=\"y_test\")\n",
    "y_pred.plot(ax=ax, label=\"seasonal naïve pred\")\n",
    "ax.set_ylabel(\"Number of airline passengers\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc295f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting predictions and past data\n",
    "plot_series(y_test, y_pred, labels=[\"y\", \"y_pred\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f47cf53",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_absolute_percentage_error(y_test, y_pred, symmetric=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f21bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the drift forecaster\n",
    "forecaster = NaiveForecaster(strategy=\"drift\")\n",
    "forecaster.fit(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e789a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = forecaster.predict(fh=fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6621698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting predictions and past data\n",
    "plot_series(y_test, y_pred, labels=[\"y\", \"y_pred\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a82934",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_absolute_percentage_error(y_test, y_pred, symmetric=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0368aa17",
   "metadata": {},
   "source": [
    "# ARIMA\n",
    "\n",
    "## Stationarity\n",
    "\n",
    "Stationarity is a fundamental concept in time series analysis. A time series is said to be stationary if its statistical properties such as mean, variance, and autocorrelation are constant (invariant) over time. In other words, it maintains a consistent structure over time, which makes it easier to model and predict.\n",
    "\n",
    "### Types of Stationarity\n",
    "\n",
    "- **Strict Stationarity**: All statistical properties of the time series are invariant to time shifts.\n",
    "- **Weak Stationarity**: Only the first two moments (mean and variance) and the autocovariance function do not change over time.\n",
    "\n",
    "### Testing for Stationarity\n",
    "\n",
    "Common tests for stationarity include the Augmented Dickey-Fuller (ADF) test and the Kwiatkowski-Phillips-Schmidt-Shin (KPSS) test. These tests assess whether a time series has a unit root, a characteristic of a non-stationary series.\n",
    "\n",
    "## Differencing\n",
    "\n",
    "Differencing is a method used to transform a non-stationary time series into a stationary one. This is done by subtracting the previous observation from the current observation.\n",
    "\n",
    "### First Differencing\n",
    "\n",
    "The first difference of a time series is the series of changes from one period to the next. It can be expressed as:\n",
    "\n",
    "$$ \\Delta y_t = y_t - y_{t-1} $$\n",
    "\n",
    "where:\n",
    "- $ \\Delta $ is the difference operator,\n",
    "- $ y_t $ is the value at time $ t $,\n",
    "- $ y_{t-1} $ is the value at time $ t-1 $.\n",
    "\n",
    "If the time series is still not stationary after the first differencing, higher-order differencing might be necessary.\n",
    "\n",
    "### Second Differencing\n",
    "\n",
    "The second difference is the difference of the first difference:\n",
    "\n",
    "$$ \\Delta^2 y_t = \\Delta(\\Delta y_t) = (y_t - y_{t-1}) - (y_{t-1} - y_{t-2}) = y_t - 2y_{t-1} + y_{t-2} $$\n",
    "\n",
    "### Seasonal Differencing\n",
    "\n",
    "For seasonal data, a seasonal difference might be more appropriate:\n",
    "\n",
    "$$ \\Delta_m y_t = y_t - y_{t-m} $$\n",
    "\n",
    "where:\n",
    "- $ m $ is the number of periods in a season.\n",
    "\n",
    "Differencing is a crucial step in the ARIMA modeling process, as ARIMA models require stationarity. The \"I\" in ARIMA stands for \"Integrated,\" which refers to the differencing process that integrates the non-stationary series to make it stationary.\n",
    "\n",
    "## ARIMA (Autoregressive Integrated Moving Average)\n",
    "\n",
    "ARIMA is a widely used statistical method for time series forecasting that captures various standard temporal structures in time series data.\n",
    "\n",
    "### Description\n",
    "\n",
    "An ARIMA model is characterized by three primary parameters: $p$, $d$, and $q$:\n",
    "\n",
    "- $p$: The number of autoregressive terms (AR part). It refers to the number of lags of the dependent variable to be used as predictors.\n",
    "- $d$: The number of nonseasonal differences needed for stationarity (I part, for integrated). It represents the order of differencing to de-trend the time series.\n",
    "- $q$: The number of lagged forecast errors in the prediction equation (MA part, for moving average). It implies using the error terms from previous time steps as predictors.\n",
    "\n",
    "### ARIMA Model Formulation\n",
    "\n",
    "The mathematical representation of an ARIMA model is:\n",
    "\n",
    "$$ (1 - \\sum_{i=1}^{p} \\phi_i B^i)(1 - B)^d y_t = (1 + \\sum_{i=1}^{q} \\theta_i B^i) \\epsilon_t $$\n",
    "\n",
    "where:\n",
    "\n",
    "- $ y_t $: The time series at time $ t $\n",
    "- $ \\phi_i $: The parameters of the autoregressive part of the model\n",
    "- $ \\theta_i $: The parameters of the moving average part\n",
    "- $ B $: The backshift operator, where $ B^i y_t = y_{t-i} $\n",
    "- $ \\epsilon_t $: The error term (white noise) at time $ t $\n",
    "\n",
    "## SARIMA (Seasonal Autoregressive Integrated Moving Average)\n",
    "\n",
    "SARIMA is an extension of the ARIMA model that includes the ability to model seasonal effects. It is widely used for time series forecasting in data that exhibit non-stationarity due to both trend and seasonality.\n",
    "\n",
    "### Description\n",
    "\n",
    "A SARIMA model is defined by seven parameters: $p, d, q$ for the non-seasonal part, and $P, D, Q, m$ for the seasonal part:\n",
    "\n",
    "- $p$: The number of autoregressive terms (AR part).\n",
    "- $d$: The degree of differencing (I part, for integrated).\n",
    "- $q$: The number of moving average terms (MA part).\n",
    "- $P$: The number of seasonal autoregressive terms.\n",
    "- $D$: The degree of seasonal differencing.\n",
    "- $Q$: The number of seasonal moving average terms.\n",
    "- $m$: The number of periods in a season.\n",
    "\n",
    "### SARIMA Model Formulation\n",
    "\n",
    "The mathematical representation of a SARIMA model is:\n",
    "\n",
    "$$ (1 - \\sum_{i=1}^{p} \\phi_i B^i)(1 - \\sum_{i=1}^{P} \\Phi_i B^{mi})(1 - B)^d(1 - B^m)^D y_t = (1 + \\sum_{i=1}^{q} \\theta_i B^i)(1 + \\sum_{i=1}^{Q} \\Theta_i B^{mi}) \\epsilon_t $$\n",
    "\n",
    "where:\n",
    "\n",
    "- $ y_t $: The time series at time $ t $,\n",
    "- $ \\phi_i $: The parameters of the non-seasonal AR part,\n",
    "- $ \\Phi_i $: The parameters of the seasonal AR part,\n",
    "- $ \\theta_i $: The parameters of the non-seasonal MA part,\n",
    "- $ \\Theta_i $: The parameters of the seasonal MA part,\n",
    "- $ B $: The backshift operator, $ B^i y_t = y_{t-i} $,\n",
    "- $ \\epsilon_t $: The error term (white noise) at time $ t $.\n",
    "\n",
    "### Seasonal Differencing\n",
    "\n",
    "Seasonal differencing is a key step in making a time series stationary on seasonal terms. It involves computing the difference between an observation and its counterpart in the previous cycle:\n",
    "\n",
    "$$ \\nabla^D y_t = (1 - B^m)^D y_t $$\n",
    "\n",
    "where $ \\nabla^D $ is the seasonal difference operator of order $ D $ and $ m $ is the seasonal period.\n",
    "\n",
    "### Non-Seasonal Differencing\n",
    "\n",
    "Non-seasonal differencing is used to stabilize the mean of the time series and is given by:\n",
    "\n",
    "$$ \\Delta^d y_t = (1 - B)^d y_t $$\n",
    "\n",
    "where $ \\Delta^d $ is the difference operator of order $ d $.\n",
    "\n",
    "### Combining Differencing\n",
    "\n",
    "The combined differencing for SARIMA is then:\n",
    "\n",
    "$$ \\Delta^d \\nabla^D y_t = (1 - B)^d(1 - B^m)^D y_t $$\n",
    "\n",
    "This differencing ensures that the time series is stationary both in terms of the trend and seasonality, preparing it for the application of ARIMA modeling techniques.\n",
    "\n",
    "## AutoARIMA\n",
    "\n",
    "AutoARIMA stands for \"Automatic ARIMA\" and is used to automate the process of ARIMA model selection by searching over various combinations of $p$, $d$, and $q$ values to find the best fitting model.\n",
    "\n",
    "### Description\n",
    "\n",
    "AutoARIMA simplifies the model-building process by conducting a search over specified parameter ranges and selecting the best model based on a given criterion, typically the Akaike Information Criterion (AIC) or the Bayesian Information Criterion (BIC).\n",
    "\n",
    "### AutoARIMA Process\n",
    "\n",
    "AutoARIMA typically involves the following steps:\n",
    "\n",
    "1. **Differencing**: Test the stationarity of the data and determine the order of differencing ($d$) required.\n",
    "2. **Model Search**: Explore different combinations of $p$ and $q$ within specified ranges.\n",
    "3. **Model Selection**: Choose the best model based on a statistical criterion, such as AIC or BIC.\n",
    "4. **Model Validation**: Validate the selected model using diagnostics checks or cross-validation.\n",
    "\n",
    "AutoARIMA can also include a seasonal component, leading to a SARIMA (Seasonal ARIMA) model, which accounts for seasonality in the data.\n",
    "\n",
    "By automating the selection process, AutoARIMA makes it easier to develop a robust forecasting model without the need for manual trial and error in selecting the ARIMA parameters. It's particularly useful for practitioners who may not have extensive experience in time series modeling or for large-scale forecasting tasks where manual model selection is impractical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8e73dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.forecasting.arima import ARIMA, AutoARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cd044c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the ARIMA parameters\n",
    "p, d, q = 1, 1, 1\n",
    "P, D, Q, m = 1, 1, 1, 12\n",
    "\n",
    "# Initialize the ARIMA model\n",
    "arima_model = ARIMA(order=(p, d, q), seasonal_order=(P, D, Q, m), suppress_warnings=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c777fa9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "arima_model.fit(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f4aaeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the AutoARIMA model\n",
    "auto_arima_model = AutoARIMA(sp=12, suppress_warnings=True, stepwise=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ffd23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "auto_arima_model.fit(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9d4f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_arima_model.get_fitted_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b03028",
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_arima_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791b20b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_arima = arima_model.predict(fh=fh)\n",
    "y_pred_auto_arima = auto_arima_model.predict(fh=fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448db929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate sMAPE for each model\n",
    "smape_arima = mean_absolute_percentage_error(y_test, y_pred_arima, symmetric=True)\n",
    "smape_auto_arima = mean_absolute_percentage_error(y_test, y_pred_auto_arima, symmetric=True)\n",
    "\n",
    "print(f'sMAPE (ARIMA): {smape_arima:.3f}')\n",
    "print(f'sMAPE (AutoARIMA): {smape_auto_arima:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbddd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_series(y_test, y_pred_arima, y_pred_auto_arima, labels=[\"y\", \"y_pred_arima\", \"y_pred_auto_auto_arima\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd89a9f",
   "metadata": {},
   "source": [
    "## Determining ARIMA Parameters using ACF and PACF Plots\n",
    "\n",
    "Autocorrelation Function (ACF) and Partial Autocorrelation Function (PACF) plots are essential tools for identifying the appropriate values for p, d, and q in ARIMA models:\n",
    "\n",
    "- **ACF**: Shows the correlation between a time series and its lagged values\n",
    "- **PACF**: Shows the correlation between a time series and its lagged values after removing the effects of intermediate lags\n",
    "\n",
    "### Interpreting ACF and PACF for ARIMA Parameter Selection:\n",
    "\n",
    "1. **Determining d (differencing order)**:\n",
    "   - If the time series is not stationary (ACF decays very slowly), differencing is needed (d > 0)\n",
    "   - Apply differencing until the series becomes stationary (ACF drops quickly)\n",
    "\n",
    "2. **Determining p (AR order) and q (MA order)**:\n",
    "   - For AR(p) processes: PACF cuts off after lag p, while ACF tails off gradually\n",
    "   - For MA(q) processes: ACF cuts off after lag q, while PACF tails off gradually\n",
    "   - For ARMA(p,q) processes: Both ACF and PACF tail off gradually\n",
    "\n",
    "Let's generate and analyze these plots for our airline passenger data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf20a057",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.stattools import adfuller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a3b8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check stationarity of the original series\n",
    "result = adfuller(y_train)\n",
    "print(f'ADF Statistic: {result[0]:.4f}')\n",
    "print(f'p-value: {result[1]:.4f}')\n",
    "print('Critical Values:')\n",
    "for key, value in result[4].items():\n",
    "    print(f'\\t{key}: {value:.4f}')\n",
    "\n",
    "# Interpret the results\n",
    "if result[1] > 0.05:\n",
    "    print(\"\\nThe series is non-stationary (fail to reject H0)\")\n",
    "    print(\"Differencing may be needed (d > 0)\")\n",
    "else:\n",
    "    print(\"\\nThe series is stationary (reject H0)\")\n",
    "    print(\"Differencing may not be needed (d = 0)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bccec99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ACF and PACF for the original series\n",
    "fig, axes = plt.subplots(2, 1, figsize=(12, 10))\n",
    "\n",
    "# ACF plot\n",
    "plot_acf(y_train, lags=36, ax=axes[0])\n",
    "axes[0].set_title('Autocorrelation Function (ACF) for Original Series')\n",
    "\n",
    "# PACF plot\n",
    "plot_pacf(y_train, lags=36, ax=axes[1])\n",
    "axes[1].set_title('Partial Autocorrelation Function (PACF) for Original Series')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0866526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply first differencing to make the series more stationary\n",
    "y_train_diff1 = y_train.diff().dropna()\n",
    "\n",
    "# Check stationarity after first differencing\n",
    "result_diff1 = adfuller(y_train_diff1)\n",
    "print(f'ADF Statistic (First Differencing): {result_diff1[0]:.4f}')\n",
    "print(f'p-value: {result_diff1[1]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059fc9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ACF and PACF for the first-differenced series\n",
    "fig, axes = plt.subplots(2, 1, figsize=(12, 10))\n",
    "\n",
    "# ACF plot for differenced series\n",
    "plot_acf(y_train_diff1, lags=36, ax=axes[0])\n",
    "axes[0].set_title('ACF for First-Differenced Series')\n",
    "\n",
    "# PACF plot for differenced series\n",
    "plot_pacf(y_train_diff1, lags=36, ax=axes[1])\n",
    "axes[1].set_title('PACF for First-Differenced Series')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db943b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply seasonal differencing (12 months) after first differencing\n",
    "y_train_diff_seasonal = y_train_diff1.diff(12).dropna()\n",
    "\n",
    "# Check stationarity after seasonal differencing\n",
    "result_diff_seasonal = adfuller(y_train_diff_seasonal)\n",
    "print(f'ADF Statistic (After Seasonal Differencing): {result_diff_seasonal[0]:.4f}')\n",
    "print(f'p-value: {result_diff_seasonal[1]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13202a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ACF and PACF for the seasonally-differenced series\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 12))\n",
    "\n",
    "# ACF plot for seasonally differenced series\n",
    "plot_acf(y_train_diff_seasonal, lags=36, ax=axes[0], alpha=0.05)\n",
    "axes[0].set_title('ACF for First and Seasonally Differenced Series')\n",
    "axes[0].axhline(y=-1.96/np.sqrt(len(y_train_diff_seasonal)), linestyle='--', color='gray')\n",
    "axes[0].axhline(y=1.96/np.sqrt(len(y_train_diff_seasonal)), linestyle='--', color='gray')\n",
    "\n",
    "# PACF plot for seasonally differenced series\n",
    "plot_pacf(y_train_diff_seasonal, lags=36, ax=axes[1], alpha=0.05, method='ywm')\n",
    "axes[1].set_title('PACF for First and Seasonally Differenced Series')\n",
    "axes[1].axhline(y=-1.96/np.sqrt(len(y_train_diff_seasonal)), linestyle='--', color='gray')\n",
    "axes[1].axhline(y=1.96/np.sqrt(len(y_train_diff_seasonal)), linestyle='--', color='gray')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419d76e8",
   "metadata": {},
   "source": [
    "##  ACF/PACF Analysis for ARIMA Parameter Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6377ecb0",
   "metadata": {},
   "source": [
    "### Interpreting the Results\n",
    "\n",
    "Based on our analysis:\n",
    "\n",
    "1. **Differencing Orders (d, D)**:\n",
    "   - First differencing (d=1) was necessary to remove the trend\n",
    "   - Seasonal differencing (D=1 with m=12) addressed the yearly seasonality\n",
    "\n",
    "2. **ACF/PACF Analysis**:\n",
    "   - The ACF shows significant spikes at lags 1 and 12, suggesting possible MA(1) and seasonal MA(1) components\n",
    "   - The PACF shows significant spikes at lags 1 and 12, suggesting possible AR(1) and seasonal AR(1) components\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e708ec",
   "metadata": {},
   "source": [
    "# Introduction to Exponential Smoothing and ETS Models\n",
    "\n",
    "## Exponential Smoothing\n",
    "\n",
    "Exponential smoothing is a family of forecasting methods that apply weighted averages of past observations to predict future values. The weights decrease exponentially as observations get older, hence the name. This approach is particularly effective for data with no clear trend or seasonal pattern.\n",
    "\n",
    "### Simple Exponential Smoothing (SES)\n",
    "\n",
    "Simple Exponential Smoothing is used when the time series is stationary, without trend or seasonality. The forecasts are calculated using weighted averages where the weights decrease exponentially:\n",
    "\n",
    "$$ \\hat{y}_{t+1} = \\alpha y_t + (1 - \\alpha) \\hat{y}_t $$\n",
    "\n",
    "where:\n",
    "- $ \\hat{y}_{t+1} $ is the forecast for the next period,\n",
    "- $ y_t $ is the actual value at time $ t $,\n",
    "- $ \\hat{y}_t $ is the forecasted value at time $ t $,\n",
    "- $ \\alpha $ is the smoothing parameter, $ 0 \\leq \\alpha \\leq 1 $.\n",
    "\n",
    "### Holt's Linear Trend Method\n",
    "\n",
    "Holt extended SES to allow forecasting data with a trend. This method has two equations: one for the level and one for the trend:\n",
    "\n",
    "Level equation:\n",
    "$$ \\ell_t = \\alpha y_t + (1 - \\alpha)(\\ell_{t-1} + b_{t-1}) $$\n",
    "\n",
    "Trend equation:\n",
    "$$ b_t = \\beta^*(\\ell_t - \\ell_{t-1}) + (1 - \\beta^*)b_{t-1} $$\n",
    "\n",
    "Forecast equation:\n",
    "$$ \\hat{y}_{t+h} = \\ell_t + hb_t $$\n",
    "\n",
    "where:\n",
    "- $ \\ell_t $ is the level estimate at time $ t $,\n",
    "- $ b_t $ is the trend estimate at time $ t $,\n",
    "- $ \\beta^* $ is the trend smoothing parameter.\n",
    "\n",
    "### Holt-Winters Seasonal Method\n",
    "\n",
    "The Holt-Winters method extends Holt's method to capture seasonality. It includes a seasonal component in addition to level and trend:\n",
    "\n",
    "Seasonal equation:\n",
    "$$ s_t = \\gamma(y_t - \\ell_{t-1} - b_{t-1}) + (1 - \\gamma)s_{t-m} $$\n",
    "\n",
    "where:\n",
    "- $ s_t $ is the seasonal component at time $ t $,\n",
    "- $ \\gamma $ is the seasonal smoothing parameter,\n",
    "- $ m $ is the length of the season.\n",
    "\n",
    "## ETS (Exponential Smoothing State Space Model) Models\n",
    "\n",
    "ETS models are a more formal statistical approach to exponential smoothing that explicitly models error, trend, and seasonal components.\n",
    "\n",
    "### Description\n",
    "\n",
    "ETS models categorize each component into additive or multiplicative, and each component can be present or absent. This leads to a variety of possible models, each suited to different kinds of time series data.\n",
    "\n",
    "### ETS Model Components\n",
    "\n",
    "The ETS model can be expressed in a state space form, allowing for a unified treatment of various exponential smoothing methods. Each model can be specified using a three-character string or acronym that represents the error, trend, and seasonal components, with each component being either:\n",
    "\n",
    "- **A** (Additive)\n",
    "- **M** (Multiplicative)\n",
    "- **N** (None)\n",
    "\n",
    "For example, an \"ANN\" model would have an Additive error, No trend, and No seasonality, while an \"MAM\" model would have a Multiplicative error, Additive trend, and Multiplicative seasonality.\n",
    "\n",
    "### ETS Model Formulation\n",
    "\n",
    "The general state space form of an ETS model can be written as:\n",
    "\n",
    "$$ y_t = \\text{Error}_t(\\text{Trend}_t(\\text{Seasonality}_t \\circ d_t)) + \\epsilon_t $$\n",
    "\n",
    "where:\n",
    "- $ y_t $ is the observed value at time $ t $,\n",
    "- $ d_t $ is the demand (level) at time $ t $,\n",
    "- $ \\epsilon_t $ is the error term at time $ t $,\n",
    "- $ \\circ $ represents the operation defined by the model (addition or multiplication),\n",
    "- The error, trend, and seasonality components are defined according to the selected ETS model.\n",
    "\n",
    "For example, an ETS(A, A, A) model has additive error, trend, and seasonality components, while an ETS(M, A, M) model has multiplicative error and seasonality but an additive trend.\n",
    "\n",
    "ETS models are fit to data using maximum likelihood estimation, and the chosen model can be used to produce point forecasts and prediction intervals.\n",
    "\n",
    "## AutoETS\n",
    "\n",
    "AutoETS refers to the automated selection and fitting of an Exponential Smoothing State Space Model (ETS). It is designed to identify the most appropriate error, trend, and seasonality (ETS) components of a time series without manual intervention.\n",
    "\n",
    "### Description\n",
    "\n",
    "AutoETS simplifies the model-building process by automatically determining the best-fitting ETS model from all possible combinations of error, trend, and seasonal components. This is particularly useful when the practitioner does not have a priori knowledge about which specific ETS model is most suitable for the data at hand.\n",
    "\n",
    "The autoETS algorithm generally involves the following steps:\n",
    "\n",
    "1. **Model Identification**: The algorithm evaluates various ETS models with different combinations of additive and multiplicative error, trend, and seasonal components.\n",
    "   \n",
    "2. **Model Selection**: It selects the best model based on a chosen information criterion, typically the Akaike Information Criterion (AIC), Bayesian Information Criterion (BIC), or Hannan-Quinn Information Criterion (HQIC).\n",
    "   \n",
    "3. **Parameter Estimation**: Once the model is selected, the algorithm estimates the smoothing parameters and initial states that best fit the historical data.\n",
    "   \n",
    "4. **Model Validation**: The fitted model can be assessed using diagnostic checks like residual analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d529c83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.api import SimpleExpSmoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003154bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate some example data\n",
    "np.random.seed(0)\n",
    "time = np.arange(100)\n",
    "values = 20 + np.random.randn(100).cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa67050b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.Series(values, index=time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff52ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SimpleExpSmoothing model\n",
    "model = SimpleExpSmoothing(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036bcd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model with smoothing_level alpha=0.2\n",
    "fit_model = model.fit(smoothing_level=0.2, optimized=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb302140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forecast the next 10 steps ahead\n",
    "forecast = fit_model.forecast(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7269ad9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(data, marker='o', color='blue', label='Actual')\n",
    "plt.plot(fit_model.fittedvalues, marker='o', color='red', label='Fitted')\n",
    "plt.plot(np.arange(100, 110), forecast, marker='o', color='green', label='Forecast')\n",
    "plt.title('Simple Exponential Smoothing')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5c1af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.forecasting.exp_smoothing import ExponentialSmoothing\n",
    "from sktime.forecasting.ets import AutoETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdac6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and fit the ETS model\n",
    "ets_model = ExponentialSmoothing(trend='additive', seasonal='multiplicative', sp=12)\n",
    "ets_model.fit(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635213bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_ets = ets_model.predict(fh=fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8213b30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_ets_model = AutoETS(auto=True, sp=12, n_jobs=-1)\n",
    "auto_ets_model.fit(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2132ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_ets_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30012fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forecast the next 36 months\n",
    "y_pred_auto_ets = auto_ets_model.predict(fh=fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659bb3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate sMAPE for each model\n",
    "smape_ets = mean_absolute_percentage_error(y_test, y_pred_ets, symmetric=True)\n",
    "smape_auto_ets = mean_absolute_percentage_error(y_test, y_pred_auto_ets, symmetric=True)\n",
    "\n",
    "print(f'sMAPE (ETS): {smape_ets:.3f}')\n",
    "print(f'sMAPE (AutoETS): {smape_auto_ets:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352b8e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_series(y_test, y_pred_ets, y_pred_auto_ets, labels=[\"y\", \"y_pred_ets\", \"y_pred_auto_ets\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
